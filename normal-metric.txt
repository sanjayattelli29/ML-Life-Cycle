"""
Comprehensive Flask API for Data Quality Metrics.
Provides extensive data quality analysis for CSV datasets.
Updated to handle both GET and POST requests.
"""
from flask import Flask, request, jsonify
from flask_cors import CORS
import pandas as pd
import numpy as np
from io import StringIO
import random
import warnings
from scipy import stats
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_selection import mutual_info_classif, mutual_info_regression
from sklearn.metrics import silhouette_score
from sklearn.cluster import KMeans
from datetime import datetime
import os
import re
from difflib import SequenceMatcher
from sklearn.metrics.pairwise import euclidean_distances
from sklearn.cluster import DBSCAN
from sklearn.neighbors import LocalOutlierFactor
from sklearn.svm import OneClassSVM

app = Flask(__name__)
CORS(app)  # Enable CORS for all routes
warnings.filterwarnings('ignore')

class DataQualityMetrics:
    """
    A comprehensive class to calculate data quality metrics for any tabular dataset.
    The class handles various data types and outputs metrics in standardized format.
    """
    
    def __init__(self, dataset_id=None, random_seed=42):
        """
        Initialize the DataQualityMetrics class.
        
        Parameters:
        -----------
        dataset_id : str, optional
            Identifier for the dataset
        random_seed : int, default=42
            Random seed for reproducibility
        """
        self.dataset_id = dataset_id or f'DS_{random.randint(1, 999):03d}'
        self.random_seed = random_seed
        np.random.seed(random_seed)
        random.seed(random_seed)

    def _is_date_column(self, series):
        """Check if a column contains date values."""
        try:
            pd.to_datetime(series.dropna().head())
            return True
        except:
            return False
    
    def _infer_expected_type(self, col, series):
        col_lower = str(col).lower()
        sample = series.dropna().astype(str)
        if len(sample) > 20:
            sample = sample.sample(20, random_state=42)
        # Email detection
        email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
        if 'email' in col_lower or sample.str.match(email_pattern, na=False).mean() > 0.7:
            return 'email'
        # Phone detection
        phone_pattern = r'^\+?1?-?\.?\s?\(?\d{3}\)?[\s.-]?\d{3}[\s.-]?\d{4}$'
        if 'phone' in col_lower or sample.str.match(phone_pattern, na=False).mean() > 0.7:
            return 'phone'
        # Date detection
        if 'date' in col_lower or pd.api.types.is_datetime64_any_dtype(series):
            return 'date'
        try:
            parsed = pd.to_datetime(sample, errors='coerce')
            if parsed.notna().mean() > 0.7:
                return 'date'
        except Exception:
            pass
        # Numeric
        if pd.api.types.is_numeric_dtype(series):
            return 'numeric'
        # Boolean
        if pd.api.types.is_bool_dtype(series):
            return 'boolean'
        # String
        if pd.api.types.is_string_dtype(series):
            return 'string'
        return 'unknown'
    
    def _detect_column_types(self, df):
        """
        Robustly detect numeric and date columns using modern pandas logic.
        Returns lists of numeric, categorical, and date columns.
        """
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        date_cols = df.select_dtypes(include=["datetime64[ns]"]).columns.tolist()
        # Try to coerce object columns to dates
        for col in df.columns:
            if col not in numeric_cols and col not in date_cols:
                coerced = pd.to_datetime(df[col], errors='coerce')
                if coerced.notna().sum() > 0 and coerced.notna().mean() > 0.7:
                    date_cols.append(col)
        categorical_cols = [col for col in df.columns if col not in numeric_cols and col not in date_cols]
        return numeric_cols, categorical_cols, date_cols
    
    def calculate_missing_values_pct(self, df):
        """Calculate percentage of missing values."""
        try:
            total_cells = df.size
            missing_cells = df.isna().sum().sum()
            return round((missing_cells / total_cells) * 100, 2)
        except Exception as e:
            return 0
    
    def calculate_duplicate_records_count(self, df):
        """Count duplicate records."""
        try:
            return len(df[df.duplicated()])
        except Exception as e:
            return 0
    
    def calculate_outlier_rate(self, df, numeric_cols):
        """Calculate outlier rate using IsolationForest."""
        try:
            if not numeric_cols or len(df) < 2:
                return 0
                
            clf = IsolationForest(contamination=0.1, random_state=self.random_seed)
            outliers = clf.fit_predict(df[numeric_cols].fillna(df[numeric_cols].mean()))
            return round(((outliers == -1).sum() / len(df)) * 100, 2)
        except Exception as e:
            return 0
    
    def calculate_inconsistency_rate(self, df):
        """Calculate data inconsistency rate."""
        try:
            inconsistencies = 0
            total_checks = 0
            
            # Check for negative values in typically positive columns
            for col in df.select_dtypes(include=['int64', 'float64']).columns:
                if any(x in col.lower() for x in ['age', 'price', 'count', 'amount', 'quantity']):
                    inconsistencies += ((df[col] < 0) | (df[col] > 120)).sum()
                    total_checks += len(df[col].dropna())
            
            return round((inconsistencies / total_checks * 100), 2) if total_checks > 0 else 0
        except Exception as e:
            return 0
    
    def calculate_data_type_mismatch_rate(self, df):
        """Calculate data type mismatch rate."""
        try:
            mismatches = 0
            total_cells = 0
            
            for col in df.select_dtypes(include=['object']).columns:
                # Try converting to numeric
                numeric_conversion = pd.to_numeric(df[col], errors='coerce')
                if not numeric_conversion.isna().all() and numeric_conversion.isna().sum() > 0:
                    mismatches += numeric_conversion.isna().sum()
                total_cells += len(df[col].dropna())
            
            return round((mismatches / total_cells * 100), 2) if total_cells > 0 else 0
        except Exception as e:
            return 0
    
    def calculate_null_vs_nan_distribution(self, df):
        """Calculate NULL vs NaN distribution."""
        try:
            null_count = df.isna().sum().sum()
            total_cells = df.size
            return round((null_count / total_cells * 100), 2) if total_cells > 0 else 0
        except Exception as e:
            return 0
    
    def calculate_cardinality_categorical(self, df, categorical_cols):
        """Calculate average cardinality of categorical columns."""
        try:
            if not categorical_cols:
                return 0
            
            cardinalities = [df[col].nunique() for col in categorical_cols]
            return round(sum(cardinalities) / len(categorical_cols), 2)
        except Exception as e:
            return 0
    
    def calculate_target_imbalance(self, df, target_col=None):
        """Calculate target variable imbalance."""
        try:
            if target_col and target_col in df.columns:
                value_counts = df[target_col].value_counts()
                min_freq = value_counts.min()
                max_freq = value_counts.max()
                return min_freq / max_freq
            return 1.0  # Perfect balance if no target
        except Exception as e:
            return 0
    
    def calculate_feature_correlation_mean(self, df, numeric_cols):
        """Calculate mean feature correlation."""
        try:
            if len(numeric_cols) < 2:
                return 0
                
            correlation_matrix = df[numeric_cols].corr().abs()
            upper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))
            return round(upper_triangle.mean().mean(), 2)
        except Exception as e:
            return 0
    
    def calculate_range_violation_rate(self, df, numeric_cols):
        """Calculate range violation rate based on z-score."""
        try:
            if not numeric_cols:
                return 0
                
            violations = 0
            total = 0
            z_threshold = 3
            
            for col in numeric_cols:
                z_scores = np.abs(stats.zscore(df[col].dropna()))
                violations += (z_scores > z_threshold).sum()
                total += len(z_scores)
            
            return round((violations / total * 100), 2) if total > 0 else 0
        except Exception as e:
            return 0
    
    def calculate_mean_median_drift(self, df, numeric_cols):
        """Calculate mean-median drift."""
        try:
            if not numeric_cols:
                return 0
                
            drifts = []
            for col in numeric_cols:
                series = df[col].dropna()
                if len(series) > 0:
                    mean = series.mean()
                    median = series.median()
                    max_val = max(abs(mean), abs(median))
                    if max_val != 0:
                        drift = abs(mean - median) / max_val
                        drifts.append(drift)
            
            return round(np.mean(drifts) * 100, 2) if drifts else 0
        except Exception as e:
            return 0
    
    def calculate_class_overlap_score(self, df, numeric_cols, target_col):
        """Calculate class overlap score using silhouette score."""
        try:
            if not numeric_cols or not target_col or target_col not in df.columns:
                return 0
                
            X = df[numeric_cols].fillna(df[numeric_cols].mean())
            y = df[target_col]
            
            if len(np.unique(y)) < 2:
                return 0
                
            score = silhouette_score(X, y)
            return round((score + 1) / 2 * 100, 2)  # Normalize to 0-100 scale
        except Exception as e:
            return 0
    
    def calculate_data_freshness(self, df, date_cols):
        """Calculate data freshness in days."""
        try:
            if not date_cols:
                return 0
                
            max_dates = []
            current_date = pd.Timestamp.now()
            
            for col in date_cols:
                dates = pd.to_datetime(df[col], errors='coerce')
                if not dates.empty and dates.notna().any():
                    max_dates.append(current_date - dates.max())
            
            if not max_dates:
                return 0
                
            avg_age = np.mean([d.days for d in max_dates])
            # Convert to a 0-100 scale where 0 days = 100 and 365+ days = 0
            freshness_score = max(0, min(100, (365 - avg_age) / 365 * 100))
            return round(freshness_score, 2)
        except Exception as e:
            return 0
    
    def calculate_feature_importance_consistency(self, df, numeric_cols, target_col=None):
        """Calculate feature importance consistency."""
        try:
            if not target_col or not numeric_cols or target_col not in df.columns:
                return 0
                
            X = df[numeric_cols]
            y = df[target_col]
            
            if isinstance(y.iloc[0], (int, float)):
                importance_scores = mutual_info_regression(X, y)
            else:
                importance_scores = mutual_info_classif(X, y)
            
            # Calculate consistency as normalized entropy of importance scores
            importance_scores = np.array(importance_scores) + 1e-10  # Avoid log(0)
            importance_probs = importance_scores / importance_scores.sum()
            entropy = -np.sum(importance_probs * np.log2(importance_probs))
            max_entropy = np.log2(len(numeric_cols))
            
            return 1 - (entropy / max_entropy) if max_entropy > 0 else 0
        except Exception as e:
            return 0
    
    def calculate_anomaly_count(self, df, numeric_cols):
        """Calculate number of anomalies."""
        try:
            if not numeric_cols:
                return 0
            clf = IsolationForest(contamination=0.05, random_state=self.random_seed)
            outliers = clf.fit_predict(df[numeric_cols])
            return (outliers == -1).sum()
        except Exception as e:
            return 0
    
    def calculate_encoding_coverage_rate(self, df, categorical_cols):
        """Calculate encoding coverage rate."""
        try:
            if not categorical_cols:
                return 1.0
                
            coverage_rates = []
            for col in categorical_cols:
                le = LabelEncoder()
                try:
                    le.fit_transform(df[col].astype(str))
                    coverage_rates.append(1.0)
                except:
                    coverage_rates.append(0.0)
            
            return np.mean(coverage_rates)
        except Exception as e:
            return 0
    
    def calculate_variance_threshold_check(self, df, numeric_cols):
        """Calculate variance threshold check."""
        try:
            if not numeric_cols:
                return 0
            
            low_variance_count = 0
            for col in numeric_cols:
                variance = df[col].var()
                if variance < 0.01:  # Threshold for low variance
                    low_variance_count += 1
            
            return low_variance_count / len(numeric_cols) if numeric_cols else 0
        except Exception as e:
            return 0
    
    def calculate_data_density_completeness(self, df):
        """Calculate data density completeness."""
        try:
            total_cells = df.size
            filled_cells = total_cells - df.isna().sum().sum()
            return filled_cells / total_cells
        except Exception as e:
            return 0
    
    def calculate_label_noise_rate(self, df, target_col=None):
        """Calculate label noise rate."""
        try:
            if not target_col or target_col not in df.columns:
                return 0
                
            # Use clustering to detect potential label noise
            if df[target_col].dtype in ['int64', 'float64']:
                n_clusters = min(3, len(df[target_col].unique()))
                kmeans = KMeans(n_clusters=n_clusters, random_state=self.random_seed)
                cluster_labels = kmeans.fit_predict(df[[target_col]])
                
                # Calculate the proportion of points far from cluster centers
                distances = kmeans.transform(df[[target_col]])
                threshold = np.percentile(distances.min(axis=1), 95)
                noise_rate = (distances.min(axis=1) > threshold).mean()
                
                return noise_rate
            return 0
        except Exception as e:
            return 0
    
    def calculate_domain_constraint_violations(self, df):
        """Calculate domain constraint violations."""
        try:
            violations = 0
            total_checks = 0
            
            for col in df.columns:
                col_lower = col.lower()
                
                # Check numeric constraints
                if df[col].dtype in ['int64', 'float64']:
                    if 'age' in col_lower:
                        violations += ((df[col] < 0) | (df[col] > 120)).sum()
                    elif 'percentage' in col_lower or 'pct' in col_lower:
                        violations += ((df[col] < 0) | (df[col] > 100)).sum()
                    elif any(word in col_lower for word in ['price', 'amount', 'cost']):
                        violations += (df[col] < 0).sum()
                    total_checks += len(df[col])
                
                # Check date constraints
                elif self._is_date_column(df[col]):
                    dates = pd.to_datetime(df[col], errors='coerce')
                    violations += ((dates < '1900-01-01') | (dates > datetime.now())).sum()
                    total_checks += len(df[col])
            
            return (violations / total_checks) * 100 if total_checks > 0 else 0
        except Exception as e:
            return 0

    def enhanced_missing_detection(self, df):
        patterns = ['', ' ', 'NULL', 'null', 'N/A', 'n/a', 'None', 'none', 
                    'missing', 'MISSING', '-', '--', '?', 'unknown', 'UNKNOWN']
        total_missing = 0
        for col in df.columns:
            missing_count = df[col].isna().sum()
            if df[col].dtype == 'object':
                for pattern in patterns:
                    missing_count += (df[col].astype(str).str.strip() == pattern).sum()
            elif df[col].dtype in ['int64', 'float64']:
                if df[col].dropna().shape[0] > 0:
                    outliers = np.abs(stats.zscore(df[col].dropna())) > 4
                    missing_count += outliers.sum()
            total_missing += missing_count
        return (total_missing / df.size) * 100 if df.size > 0 else 0

    def enhanced_duplicate_detection(self, df):
        exact_dupes = len(df[df.duplicated()])
        fuzzy_dupes = 0
        for col in df.select_dtypes(include=['object']).columns:
            values = df[col].dropna().unique()
            for i, val1 in enumerate(values):
                for val2 in values[i+1:]:
                    similarity = SequenceMatcher(None, str(val1), str(val2)).ratio()
                    if 0.8 <= similarity < 1.0:
                        fuzzy_dupes += 1
        near_dupes = 0
        numeric_cols = df.select_dtypes(include=['number']).columns
        if len(numeric_cols) > 0 and df.shape[0] > 1:
            distances = euclidean_distances(df[numeric_cols].fillna(0))
            threshold = np.percentile(distances, 5)
            near_dupes = (distances < threshold).sum() - len(df)
        return {
            'exact_duplicates': exact_dupes,
            'fuzzy_duplicates': fuzzy_dupes,
            'near_duplicates': near_dupes,
            'total_duplicates': exact_dupes + fuzzy_dupes + near_dupes
        }

    def enhanced_outlier_detection(self, df, numeric_cols):
        outlier_methods = {}
        for col in numeric_cols:
            data = df[col].dropna()
            outliers_detected = set()
            # IQR
            Q1, Q3 = data.quantile([0.25, 0.75])
            IQR = Q3 - Q1
            iqr_outliers = data[(data < Q1 - 1.5*IQR) | (data > Q3 + 1.5*IQR)]
            outliers_detected.update(iqr_outliers.index)
            # Modified Z-score
            mad = np.median(np.abs(data - np.median(data)))
            if mad != 0:
                modified_z = 0.6745 * (data - np.median(data)) / mad
                z_outliers = data[np.abs(modified_z) > 3.5]
                outliers_detected.update(z_outliers.index)
            # Isolation Forest
            if len(data) > 10:
                iso_forest = IsolationForest(contamination=0.1, random_state=42)
                iso_predictions = iso_forest.fit_predict(data.values.reshape(-1, 1))
                iso_outliers = data[iso_predictions == -1]
                outliers_detected.update(iso_outliers.index)
            # DBSCAN
            if len(data) > 5:
                dbscan = DBSCAN(eps=np.std(data)*0.5, min_samples=3)
                clusters = dbscan.fit_predict(data.values.reshape(-1, 1))
                dbscan_outliers = data[clusters == -1]
                outliers_detected.update(dbscan_outliers.index)
            outlier_methods[col] = len(outliers_detected)
        total_outliers = sum(outlier_methods.values())
        return (total_outliers / len(df)) * 100 if len(df) > 0 else 0

    def enhanced_data_type_validation(self, df):
        type_issues = 0
        total_cells = 0
        for col in df.columns:
            expected_type = self._infer_expected_type(col, df[col])
            actual_violations = 0
            if expected_type == 'email':
                email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
                violations = ~df[col].astype(str).str.match(email_pattern, na=False)
                actual_violations = violations.sum()
            elif expected_type == 'phone':
                phone_pattern = r'^\+?1?-?\.?\s?\(?\d{3}\)?[\s.-]?\d{3}[\s.-]?\d{4}$'
                violations = ~df[col].astype(str).str.match(phone_pattern, na=False)
                actual_violations = violations.sum()
            elif expected_type == 'date':
                try:
                    actual_violations = pd.to_datetime(df[col], errors='coerce').isna().sum()
                except:
                    actual_violations = len(df[col])
            elif expected_type == 'numeric':
                violations = pd.to_numeric(df[col], errors='coerce').isna()
                actual_violations = violations.sum()
            elif expected_type == 'categorical':
                if df[col].dtype == 'object':
                    normalized = df[col].str.lower().str.strip()
                    original_unique = df[col].nunique()
                    normalized_unique = normalized.nunique()
                    actual_violations = original_unique - normalized_unique
            type_issues += actual_violations
            total_cells += len(df[col])
        return (type_issues / total_cells) * 100 if total_cells > 0 else 0

    def enhanced_correlation_analysis(self, df, numeric_cols):
        if len(numeric_cols) < 2:
            return 0
        correlations = {
            'pearson': [],
            'spearman': [],
            'kendall': [],
            'mutual_info': []
        }
        pearson_corr = df[numeric_cols].corr(method='pearson').abs()
        spearman_corr = df[numeric_cols].corr(method='spearman').abs()
        kendall_corr = df[numeric_cols].corr(method='kendall').abs()
        mask = np.triu(np.ones_like(pearson_corr, dtype=bool), k=1)
        correlations['pearson'] = pearson_corr.where(mask).stack().tolist()
        correlations['spearman'] = spearman_corr.where(mask).stack().tolist()
        correlations['kendall'] = kendall_corr.where(mask).stack().tolist()
        for i, col1 in enumerate(numeric_cols):
            for j, col2 in enumerate(numeric_cols[i+1:], i+1):
                mi_score = mutual_info_regression(
                    df[[col1]].fillna(df[col1].mean()), 
                    df[col2].fillna(df[col2].mean())
                )[0]
                correlations['mutual_info'].append(mi_score)
        weights = {'pearson': 0.3, 'spearman': 0.3, 'kendall': 0.2, 'mutual_info': 0.2}
        weighted_corr = sum(
            np.mean(correlations[method]) * weight 
            for method, weight in weights.items()
        )
        return round(weighted_corr, 4)

    def enhanced_domain_validation(self, df):
        violations = 0
        total_checks = 0
        domain_rules = {
            'age': {'min': 0, 'max': 120, 'type': 'int'},
            'salary': {'min': 0, 'max': 10000000, 'type': 'float'},
            'percentage': {'min': 0, 'max': 100, 'type': 'float'},
            'rating': {'min': 1, 'max': 5, 'type': 'float'},
            'score': {'min': 0, 'max': 100, 'type': 'float'},
            'price': {'min': 0, 'max': float('inf'), 'type': 'float'},
            'quantity': {'min': 0, 'max': float('inf'), 'type': 'int'},
            'year': {'min': 1900, 'max': 2030, 'type': 'int'},
            'month': {'min': 1, 'max': 12, 'type': 'int'},
            'day': {'min': 1, 'max': 31, 'type': 'int'},
            'hour': {'min': 0, 'max': 23, 'type': 'int'},
            'minute': {'min': 0, 'max': 59, 'type': 'int'}
        }
        for col in df.columns:
            col_lower = col.lower()
            applicable_rule = None
            for domain, rule in domain_rules.items():
                if domain in col_lower:
                    applicable_rule = rule
                    break
            if applicable_rule and df[col].dtype in ['int64', 'float64']:
                range_violations = (
                    (df[col] < applicable_rule['min']) | 
                    (df[col] > applicable_rule['max'])
                ).sum()
                if applicable_rule['type'] == 'int':
                    type_violations = (df[col] % 1 != 0).sum()
                else:
                    type_violations = 0
                violations += range_violations + type_violations
                total_checks += len(df[col])
            elif 'email' in col_lower:
                email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'
                email_violations = ~df[col].astype(str).str.match(email_pattern, na=False)
                violations += email_violations.sum()
                total_checks += len(df[col])
            elif any(word in col_lower for word in ['phone', 'mobile']):
                phone_pattern = r'^\\+?[\\d\\s\\-\\(\\)\\.]{10,}$'
                phone_violations = ~df[col].astype(str).str.match(phone_pattern, na=False)
                violations += phone_violations.sum()
                total_checks += len(df[col])
            elif any(word in col_lower for word in ['date', 'time']):
                try:
                    parsed_dates = pd.to_datetime(df[col], errors='coerce')
                    date_violations = parsed_dates.isna().sum()
                    if 'birth' in col_lower or 'dob' in col_lower:
                        future_violations = (parsed_dates > pd.Timestamp.now()).sum()
                        date_violations += future_violations
                    violations += date_violations
                    total_checks += len(df[col])
                except:
                    violations += len(df[col])
                    total_checks += len(df[col])
        return (violations / total_checks) * 100 if total_checks > 0 else 0

    def enhanced_data_freshness(self, df, date_cols):
        if not date_cols:
            return self._calculate_row_based_freshness(df)
        freshness_metrics = {}
        current_time = pd.Timestamp.now()
        for col in date_cols:
            dates = pd.to_datetime(df[col], errors='coerce').dropna()
            if len(dates) == 0:
                continue
            metrics = {}
            latest_date = dates.max()
            days_old = (current_time - latest_date).days
            recency_score = max(0, 100 - (days_old / 365 * 100))
            date_range = (dates.max() - dates.min()).days
            expected_range = 365
            coverage_score = min(100, (date_range / expected_range) * 100)
            date_diffs = dates.sort_values().diff().dt.days.dropna()
            if len(date_diffs) > 0:
                freq_consistency = 100 - (date_diffs.std() / date_diffs.mean() * 100)
                freq_consistency = max(0, min(100, freq_consistency))
            else:
                freq_consistency = 0
            if len(dates) > 1:
                expected_dates = pd.date_range(dates.min(), dates.max(), freq='D')
                actual_dates = set(dates.dt.date)
                expected_dates_set = set(expected_dates.date)
                gap_ratio = 1 - (len(actual_dates) / len(expected_dates_set))
                completeness_score = (1 - gap_ratio) * 100
            else:
                completeness_score = 0
            composite_score = (
                recency_score * 0.4 + 
                coverage_score * 0.2 + 
                freq_consistency * 0.2 + 
                completeness_score * 0.2
            )
            freshness_metrics[col] = {
                'recency': recency_score,
                'coverage': coverage_score,
                'frequency': freq_consistency,
                'completeness': completeness_score,
                'composite': composite_score
            }
        if freshness_metrics:
            overall_freshness = np.mean([m['composite'] for m in freshness_metrics.values()])
            return round(overall_freshness, 2)
        return 0

    def enhanced_anomaly_detection(self, df, numeric_cols):
        if not numeric_cols or len(df) < 10:
            return 0
        X = df[numeric_cols].fillna(df[numeric_cols].mean())
        anomaly_scores = np.zeros(len(X))
        # Isolation Forest
        iso_forest = IsolationForest(contamination=0.1, random_state=42)
        iso_scores = iso_forest.decision_function(X)
        iso_anomalies = iso_forest.predict(X) == -1
        # Local Outlier Factor
        lof = LocalOutlierFactor(contamination=0.1)
        lof_scores = lof.fit_predict(X)
        lof_anomalies = lof_scores == -1
        # One-Class SVM
        svm = OneClassSVM(gamma='scale', nu=0.1)
        svm_scores = svm.fit_predict(X)
        svm_anomalies = svm_scores == -1
        # Mahalanobis distance
        try:
            cov_matrix = np.cov(X.T)
            mean_vector = np.mean(X, axis=0)
            mahal_distances = []
            for i, row in enumerate(X.values):
                diff = row - mean_vector
                mahal_dist = np.sqrt(diff.T @ np.linalg.inv(cov_matrix) @ diff)
                mahal_distances.append(mahal_dist)
            threshold = np.percentile(mahal_distances, 90)
            mahal_anomalies = np.array(mahal_distances) > threshold
        except:
            mahal_anomalies = np.zeros(len(X), dtype=bool)
        # DBSCAN
        dbscan = DBSCAN(eps=0.5, min_samples=5)
        cluster_labels = dbscan.fit_predict(X)
        dbscan_anomalies = cluster_labels == -1
        anomaly_votes = (
            iso_anomalies.astype(int) + 
            lof_anomalies.astype(int) + 
            svm_anomalies.astype(int) + 
            mahal_anomalies.astype(int) + 
            dbscan_anomalies.astype(int)
        )
        high_confidence_anomalies = (anomaly_votes >= 3).sum()
        medium_confidence_anomalies = (anomaly_votes == 2).sum()
        total_anomalies = high_confidence_anomalies + (medium_confidence_anomalies * 0.5)
        return int(total_anomalies)

    def enhanced_row_count(self, df):
        """Enhanced row counting with validation and total_columns."""
        try:
            total_rows = len(df)
            total_columns = len(df.columns)
            empty_rows = df.isnull().all(axis=1).sum()
            meaningful_threshold = total_columns * 0.5
            meaningful_rows = (df.count(axis=1) >= meaningful_threshold).sum()
            exact_duplicates = df.duplicated().sum()
            return {
                'total_rows': total_rows,
                'total_columns': total_columns,
                'empty_rows': empty_rows,
                'meaningful_rows': meaningful_rows,
                'exact_duplicates': exact_duplicates,
                'duplicate_rows': exact_duplicates,
                'effective_rows': meaningful_rows - exact_duplicates
            }
        except Exception as e:
            return {'total_rows': 0, 'total_columns': 0, 'duplicate_rows': 0, 'error': str(e)}

    def enhanced_file_size_analysis(self, df, file_path=None, csv_data=None):
        """Comprehensive file size analysis"""
        try:
            metrics = {}
            if file_path and os.path.exists(file_path):
                actual_size = os.path.getsize(file_path)
                metrics['actual_file_size_mb'] = round(actual_size / (1024**2), 3)
            memory_usage = df.memory_usage(index=True, deep=True).sum()
            metrics['memory_footprint_mb'] = round(memory_usage / (1024**2), 3)
            if csv_data:
                raw_size = len(csv_data.encode('utf-8'))
                metrics['raw_csv_size_mb'] = round(raw_size / (1024**2), 3)
            if 'actual_file_size_mb' in metrics and 'memory_footprint_mb' in metrics:
                compression_ratio = metrics['memory_footprint_mb'] / metrics['actual_file_size_mb']
                metrics['compression_ratio'] = round(compression_ratio, 2)
            non_null_cells = df.count().sum()
            if non_null_cells > 0:
                metrics['bytes_per_data_point'] = round(memory_usage / non_null_cells, 2)
            return metrics
        except Exception as e:
            return {'error': str(e)}

    def enhanced_numeric_column_detection(self, df):
        """Robust numeric column detection and analysis"""
        try:
            results = {
                'pure_numeric': [],
                'convertible_numeric': [],
                'mixed_numeric': [],
                'pseudo_numeric': [],
                'invalid_numeric': []
            }
            pure_numeric = df.select_dtypes(include=[np.number]).columns.tolist()
            results['pure_numeric'] = pure_numeric
            for col in df.select_dtypes(include=['object']).columns:
                converted = pd.to_numeric(df[col], errors='coerce')
                conversion_rate = converted.notna().mean()
                if conversion_rate >= 0.95:
                    results['convertible_numeric'].append(col)
                elif conversion_rate >= 0.7:
                    results['mixed_numeric'].append(col)
                elif conversion_rate >= 0.3:
                    results['pseudo_numeric'].append(col)
                else:
                    numeric_pattern = df[col].astype(str).str.match(r'^[0-9]+$', na=False)
                    if numeric_pattern.mean() > 0.8:
                        results['pseudo_numeric'].append(col)
            validated_numeric = []
            for col in pure_numeric:
                issues = []
                if np.isinf(df[col]).any():
                    issues.append('infinite_values')
                if df[col].min() == df[col].max():
                    issues.append('constant_value')
                q1, q3 = df[col].quantile([0.25, 0.75])
                iqr = q3 - q1
                extreme_outliers = ((df[col] < q1 - 3*iqr) | (df[col] > q3 + 3*iqr)).sum()
                if extreme_outliers > len(df) * 0.1:
                    issues.append('extreme_outliers')
                if not issues:
                    validated_numeric.append(col)
                else:
                    results['invalid_numeric'].append({'column': col, 'issues': issues})
            total_numeric = len(results['pure_numeric']) + len(results['convertible_numeric'])
            results['summary'] = {
                'total_numeric_columns': total_numeric,
                'high_quality_numeric': len(validated_numeric),
                'needs_conversion': len(results['convertible_numeric']),
                'mixed_quality': len(results['mixed_numeric']),
                'quality_score': round(len(validated_numeric) / max(1, total_numeric), 3)
            }
            return results
        except Exception as e:
            return {'error': str(e)}

    def enhanced_date_column_detection(self, df):
        """Comprehensive date column detection and validation"""
        try:
            results = {
                'explicit_datetime': [],
                'parseable_dates': [],
                'ambiguous_dates': [],
                'invalid_dates': [],
                'date_formats': {}
            }
            explicit_datetime = df.select_dtypes(include=["datetime64[ns]", "datetime64"]).columns.tolist()
            results['explicit_datetime'] = explicit_datetime
            date_patterns = [
                (r'\d{4}-\d{2}-\d{2}', '%Y-%m-%d'),
                (r'\d{2}/\d{2}/\d{4}', '%m/%d/%Y'),
                (r'\d{2}-\d{2}-\d{4}', '%m-%d-%Y'),
                (r'\d{4}/\d{2}/\d{2}', '%Y/%m/%d'),
                (r'\d{1,2}/\d{1,2}/\d{2,4}', '%m/%d/%Y'),
                (r'\d{2}\w{3}\d{4}', '%d%b%Y'),
                (r'\w{3} \d{1,2}, \d{4}', '%b %d, %Y'),
            ]
            for col in df.columns:
                if col in explicit_datetime:
                    continue
                col_str = df[col].astype(str).str.strip()
                if col_str.str.contains(r'^[a-zA-Z\s]+$', na=False).mean() > 0.8:
                    continue
                try:
                    parsed_auto = pd.to_datetime(df[col], errors='coerce')
                    auto_success_rate = parsed_auto.notna().mean()
                    if auto_success_rate >= 0.9:
                        results['parseable_dates'].append(col)
                        results['date_formats'][col] = 'auto_detected'
                        continue
                    elif auto_success_rate >= 0.5:
                        results['ambiguous_dates'].append(col)
                        continue
                except:
                    pass
                best_pattern = None
                best_rate = 0
                for pattern, fmt in date_patterns:
                    matches = col_str.str.match(pattern, na=False)
                    match_rate = matches.mean()
                    if match_rate > best_rate and match_rate >= 0.7:
                        best_rate = match_rate
                        best_pattern = fmt
                if best_pattern:
                    try:
                        parsed_specific = pd.to_datetime(df[col], format=best_pattern, errors='coerce')
                        specific_success_rate = parsed_specific.notna().mean()
                        if specific_success_rate >= 0.8:
                            results['parseable_dates'].append(col)
                            results['date_formats'][col] = best_pattern
                        elif specific_success_rate >= 0.5:
                            results['ambiguous_dates'].append(col)
                    except:
                        results['invalid_dates'].append(col)
            validated_dates = []
            for col in results['parseable_dates'] + results['explicit_datetime']:
                issues = []
                if col in results['explicit_datetime']:
                    dates = df[col]
                else:
                    dates = pd.to_datetime(df[col], errors='coerce')
                min_date = dates.min()
                max_date = dates.max()
                current_date = pd.Timestamp.now()
                if min_date < pd.Timestamp('1900-01-01'):
                    issues.append('too_old')
                if max_date > current_date + pd.Timedelta(days=365):
                    issues.append('future_dates')
                if hasattr(dates, 'dt') and dates.dt.day.value_counts().iloc[0] > len(dates) * 0.5:
                    issues.append('day_clustering')
                if not issues:
                    validated_dates.append(col)
            results['summary'] = {
                'total_date_columns': len(results['explicit_datetime']) + len(results['parseable_dates']),
                'high_quality_dates': len(validated_dates),
                'needs_parsing': len(results['parseable_dates']),
                'ambiguous_dates': len(results['ambiguous_dates']),
                'quality_score': round(len(validated_dates) / max(1, len(results['explicit_datetime']) + len(results['parseable_dates'])), 3)
            }
            return results
        except Exception as e:
            return {'error': str(e)}

    def ultra_enhanced_missing_detection(self, df):
        """Ultra-comprehensive missing value detection"""
        try:
            missing_analysis = {
                'patterns': {},
                'by_column': {},
                'by_pattern': {},
                'correlation_analysis': {}
            }
            missing_patterns = {
                'explicit_null': [None, np.nan, pd.NaT],
                'string_nulls': ['', ' ', 'NULL', 'null', 'N/A', 'n/a', 'NA', 'na', 'None', 'none', 'NONE', 'nil', 'NIL', 'NaN', 'nan'],
                'placeholder_values': ['-', '--', '---', '?', '??', '???', 'unknown', 'UNKNOWN', 'Unknown', 'missing', 'MISSING', 'Missing'],
                'zero_like': ['0', '00', '000', '0.0', '0.00'],
                'default_dates': ['1900-01-01', '1970-01-01', '9999-12-31', '0000-00-00'],
                'suspicious_text': ['test', 'TEST', 'Test', 'dummy', 'DUMMY', 'Dummy', 'placeholder', 'PLACEHOLDER', 'Placeholder']
            }
            total_cells = df.size
            total_missing = 0
            for col in df.columns:
                col_missing = {
                    'explicit_null': 0,
                    'string_nulls': 0,
                    'placeholder_values': 0,
                    'zero_like': 0,
                    'default_dates': 0,
                    'suspicious_text': 0,
                    'statistical_outliers': 0
                }
                col_missing['explicit_null'] = df[col].isnull().sum()
                if df[col].dtype == 'object':
                    col_str = df[col].astype(str).str.strip().str.lower()
                    for pattern_type, patterns in missing_patterns.items():
                        if pattern_type == 'explicit_null':
                            continue
                        for pattern in patterns:
                            if pattern_type == 'zero_like' and not self._is_numeric_column(col, df):
                                continue
                            if pattern_type == 'default_dates' and not self._is_date_column(col, df):
                                continue
                            matches = (col_str == pattern.lower()).sum()
                            col_missing[pattern_type] += matches
                if df[col].dtype in ['int64', 'float64']:
                    try:
                        z_scores = np.abs(stats.zscore(df[col].dropna()))
                        extreme_outliers = (z_scores > 4).sum()
                        col_missing['statistical_outliers'] = extreme_outliers
                    except:
                        pass
                if df[col].dtype == 'object':
                    value_counts = df[col].value_counts()
                    if len(value_counts) > 0:
                        most_common_freq = value_counts.iloc[0]
                        if most_common_freq > len(df) * 0.8:
                            most_common_val = str(value_counts.index[0]).lower().strip()
                            if len(most_common_val) <= 3 or most_common_val in ['same', 'duplicate', 'copy']:
                                col_missing['placeholder_values'] += most_common_freq
                col_total_missing = sum(col_missing.values())
                missing_analysis['by_column'][col] = {
                    'total_missing': col_total_missing,
                    'missing_rate': round(col_total_missing / len(df) * 100, 2),
                    'breakdown': col_missing
                }
                total_missing += col_total_missing
            missing_analysis['correlation_analysis'] = self._analyze_missing_correlations(df, missing_analysis['by_column'])
            overall_missing_rate = round(total_missing / total_cells * 100, 3)
            missing_analysis['summary'] = {
                'overall_missing_rate': overall_missing_rate,
                'total_missing_values': int(total_missing),
                'columns_with_missing': sum(1 for col_data in missing_analysis['by_column'].values() if col_data['total_missing'] > 0),
                'severely_incomplete_columns': sum(1 for col_data in missing_analysis['by_column'].values() if col_data['missing_rate'] > 50),
                'data_completeness_score': round(100 - overall_missing_rate, 2)
            }
            return missing_analysis
        except Exception as e:
            return {'error': str(e), 'fallback_rate': self.calculate_missing_values_pct(df)}

    def _analyze_missing_correlations(self, df, missing_by_column):
        """Analyze correlations between missing values across columns"""
        try:
            missing_matrix = pd.DataFrame()
            for col in df.columns:
                missing_matrix[f'{col}_missing'] = df[col].isnull()
            correlations = missing_matrix.corr()
            high_correlations = []
            for i in range(len(correlations.columns)):
                for j in range(i+1, len(correlations.columns)):
                    corr_val = correlations.iloc[i, j]
                    if abs(corr_val) > 0.7:
                        high_correlations.append({
                            'column1': correlations.columns[i].replace('_missing', ''),
                            'column2': correlations.columns[j].replace('_missing', ''),
                            'correlation': round(corr_val, 3)
                        })
            return {
                'correlated_missing_patterns': high_correlations,
                'missing_correlation_matrix': correlations.round(3).to_dict()
            }
        except:
            return {'error': 'Could not analyze missing correlations'}

    def _is_numeric_column(self, col, df):
        return pd.api.types.is_numeric_dtype(df[col])

    def _is_date_column(self, col, df):
        try:
            pd.to_datetime(df[col], errors='coerce')
            return True
        except:
            return False

    def ultra_enhanced_outlier_detection(self, df, numeric_cols):
        """Multi-method outlier detection with confidence scoring"""
        try:
            if not numeric_cols or len(df) < 10:
                return {'outlier_rate': 0, 'methods_used': 0}
            outlier_results = {
                'by_column': {},
                'by_method': {},
                'confidence_levels': {},
                'summary': {}
            }
            methods = {
                'iqr': self._detect_iqr_outliers,
                'z_score': self._detect_zscore_outliers,
                'modified_z_score': self._detect_modified_zscore_outliers,
                'isolation_forest': self._detect_isolation_forest_outliers,
                'local_outlier_factor': self._detect_lof_outliers,
                'one_class_svm': self._detect_ocsvm_outliers,
                'dbscan': self._detect_dbscan_outliers
            }
            method_results = {}
            for method_name, method_func in methods.items():
                try:
                    result = method_func(df, numeric_cols)
                    method_results[method_name] = result
                    outlier_results['by_method'][method_name] = {
                        'outliers_detected': len(result),
                        'outlier_rate': round(len(result) / len(df) * 100, 2)
                    }
                except Exception as e:
                    outlier_results['by_method'][method_name] = {'error': str(e)}
            all_indices = set(range(len(df)))
            vote_counts = {idx: 0 for idx in all_indices}
            for method_outliers in method_results.values():
                for idx in method_outliers:
                    vote_counts[idx] += 1
            high_confidence = [idx for idx, votes in vote_counts.items() if votes >= 5]
            medium_confidence = [idx for idx, votes in vote_counts.items() if 3 <= votes < 5]
            low_confidence = [idx for idx, votes in vote_counts.items() if 1 <= votes < 3]
            outlier_results['confidence_levels'] = {
                'high_confidence': len(high_confidence),
                'medium_confidence': len(medium_confidence),
                'low_confidence': len(low_confidence)
            }
            total_outliers = len(high_confidence) + 0.6 * len(medium_confidence) + 0.2 * len(low_confidence)
            weighted_outlier_rate = round(total_outliers / len(df) * 100, 2)
            for col in numeric_cols:
                col_outliers = self._analyze_column_outliers(df[col], col)
                outlier_results['by_column'][col] = col_outliers
            outlier_results['summary'] = {
                'weighted_outlier_rate': weighted_outlier_rate,
                'methods_successful': len([m for m in outlier_results['by_method'].values() if 'error' not in m]),
                'consensus_outliers': len(high_confidence),
                'total_flagged_records': len([idx for idx, votes in vote_counts.items() if votes > 0])
            }
            return outlier_results
        except Exception as e:
            return {'error': str(e)}

    def _detect_iqr_outliers(self, df, numeric_cols):
        outliers = set()
        for col in numeric_cols:
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            col_outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)].index
            outliers.update(col_outliers)
        return list(outliers)

    def _detect_zscore_outliers(self, df, numeric_cols):
        outliers = set()
        for col in numeric_cols:
            z_scores = np.abs(stats.zscore(df[col].dropna()))
            threshold = 3
            col_outliers = df[col].dropna()[z_scores > threshold].index
            outliers.update(col_outliers)
        return list(outliers)

    def _detect_modified_zscore_outliers(self, df, numeric_cols):
        outliers = set()
        for col in numeric_cols:
            data = df[col].dropna()
            if len(data) == 0:
                continue
            median = np.median(data)
            mad = np.median(np.abs(data - median))
            if mad == 0:
                continue
            modified_z = 0.6745 * (data - median) / mad
            col_outliers = data[np.abs(modified_z) > 3.5].index
            outliers.update(col_outliers)
        return list(outliers)

    def _detect_isolation_forest_outliers(self, df, numeric_cols):
        outliers = set()
        if len(df) < 10:
            return []
        clf = IsolationForest(contamination=0.1, random_state=42)
        preds = clf.fit_predict(df[numeric_cols].fillna(df[numeric_cols].mean()))
        outlier_indices = df.index[preds == -1]
        outliers.update(outlier_indices)
        return list(outliers)

    def _detect_lof_outliers(self, df, numeric_cols):
        outliers = set()
        if len(df) < 10:
            return []
        lof = LocalOutlierFactor(contamination=0.1)
        preds = lof.fit_predict(df[numeric_cols].fillna(df[numeric_cols].mean()))
        outlier_indices = df.index[preds == -1]
        outliers.update(outlier_indices)
        return list(outliers)

    def _detect_ocsvm_outliers(self, df, numeric_cols):
        outliers = set()
        if len(df) < 10:
            return []
        svm = OneClassSVM(gamma='scale', nu=0.1)
        preds = svm.fit_predict(df[numeric_cols].fillna(df[numeric_cols].mean()))
        outlier_indices = df.index[preds == -1]
        outliers.update(outlier_indices)
        return list(outliers)

    def _detect_dbscan_outliers(self, df, numeric_cols):
        outliers = set()
        if len(df) < 10:
            return []
        dbscan = DBSCAN(eps=0.5, min_samples=5)
        cluster_labels = dbscan.fit_predict(df[numeric_cols].fillna(df[numeric_cols].mean()))
        outlier_indices = df.index[cluster_labels == -1]
        outliers.update(outlier_indices)
        return list(outliers)

    def _analyze_column_outliers(self, series, col):
        stats_ = {}
        try:
            stats_['min'] = float(series.min())
            stats_['max'] = float(series.max())
            stats_['mean'] = float(series.mean())
            stats_['std'] = float(series.std())
            stats_['iqr'] = float(series.quantile(0.75) - series.quantile(0.25))
            stats_['missing'] = int(series.isnull().sum())
        except:
            pass
        return stats_

    def ultra_enhanced_correlation_analysis(self, df, numeric_cols):
        """Comprehensive correlation analysis with multiple methods"""
        try:
            if len(numeric_cols) < 2:
                return {'correlation_score': 0, 'methods': 0}
            correlation_results = {
                'correlation_matrices': {},
                'statistical_tests': {},
                'correlation_strengths': {},
                'multicollinearity_analysis': {},
                'mutual_information': {},
                'summary': {}
            }
            clean_df = df[numeric_cols].fillna(df[numeric_cols].median())
            methods = ['pearson', 'spearman', 'kendall']
            for method in methods:
                try:
                    corr_matrix = clean_df.corr(method=method)
                    correlation_results['correlation_matrices'][method] = corr_matrix.round(4).to_dict()
                    mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)
                    correlations = corr_matrix.where(mask).stack()
                    correlation_results['correlation_strengths'][method] = {
                        'mean_correlation': round(correlations.abs().mean(), 4),
                        'max_correlation': round(correlations.abs().max(), 4),
                        'high_correlations': int((correlations.abs() > 0.8).sum()),
                        'moderate_correlations': int(((correlations.abs() > 0.5) & (correlations.abs() <= 0.8)).sum()),
                        'weak_correlations': int((correlations.abs() <= 0.5).sum())
                    }
                except Exception as e:
                    correlation_results['correlation_matrices'][method] = {'error': str(e)}
            try:
                from statsmodels.stats.outliers_influence import variance_inflation_factor
                vif_data = {}
                for i, col in enumerate(clean_df.columns):
                    vif_data[col] = round(variance_inflation_factor(clean_df.values, i), 2)
                correlation_results['multicollinearity_analysis']['vif'] = vif_data
                condition_indices = self._calculate_condition_indices(clean_df)
                correlation_results['multicollinearity_analysis']['condition_indices'] = condition_indices
                eigenvalues = np.linalg.eigvals(clean_df.corr())
                correlation_results['multicollinearity_analysis']['eigenvalue_analysis'] = {
                    'min_eigenvalue': round(float(np.min(eigenvalues)), 6),
                    'max_eigenvalue': round(float(np.max(eigenvalues)), 6),
                    'condition_number': round(float(np.max(eigenvalues) / np.min(eigenvalues)), 2)
                }
            except Exception as e:
                correlation_results['multicollinearity_analysis']['error'] = str(e)
            try:
                mi_scores = {}
                for i, col1 in enumerate(numeric_cols):
                    for j, col2 in enumerate(numeric_cols[i+1:], i+1):
                        mi_score = mutual_info_regression(
                            clean_df[[col1]],
                            clean_df[col2],
                            random_state=42
                        )[0]
                        mi_scores[f"{col1}_vs_{col2}"] = round(mi_score, 4)
                correlation_results['mutual_information'] = {
                    'scores': mi_scores,
                    'mean_mi': round(np.mean(list(mi_scores.values())), 4) if mi_scores else 0,
                    'max_mi': round(np.max(list(mi_scores.values())), 4) if mi_scores else 0
                }
            except Exception as e:
                correlation_results['mutual_information'] = {'error': str(e)}
            pearson_mean = correlation_results['correlation_strengths'].get('pearson', {}).get('mean_correlation', 0)
            spearman_mean = correlation_results['correlation_strengths'].get('spearman', {}).get('mean_correlation', 0)
            kendall_mean = correlation_results['correlation_strengths'].get('kendall', {}).get('mean_correlation', 0)
            mi_mean = correlation_results.get('mutual_information', {}).get('mean_mi', 0)
            combined_score = (pearson_mean * 0.4 + spearman_mean * 0.3 + kendall_mean * 0.2 + mi_mean * 0.1)
            correlation_results['summary'] = {
                'combined_correlation_score': round(combined_score, 4),
                'methods_successful': len([m for m in correlation_results['correlation_matrices'].values() if 'error' not in m]),
                'multicollinearity_risk': self._assess_multicollinearity_risk(correlation_results),
                'correlation_complexity': round(np.std([pearson_mean, spearman_mean, kendall_mean]), 4)
            }
            return correlation_results
        except Exception as e:
            return {'error': str(e)}

    def _calculate_condition_indices(self, df):
        try:
            from numpy.linalg import svd
            X = df.values
            _, s, _ = svd(X, full_matrices=False)
            condition_indices = (s[0] / s).tolist()
            return [round(float(x), 4) for x in condition_indices]
        except Exception as e:
            return {'error': str(e)}

    def _assess_multicollinearity_risk(self, correlation_results):
        try:
            vif = correlation_results['multicollinearity_analysis'].get('vif', {})
            if isinstance(vif, dict):
                high_vif = [col for col, v in vif.items() if isinstance(v, (int, float)) and v > 10]
                if high_vif:
                    return 'High risk: ' + ', '.join(high_vif)
                else:
                    return 'Low risk'
            return 'Unknown'
        except:
            return 'Unknown'

    def ultra_enhanced_range_violation_detection(self, df, numeric_cols):
        """Comprehensive range violation detection"""
        try:
            if not numeric_cols:
                return {'range_violation_rate': 0, 'violations': {}}
            violation_analysis = {
                'by_column': {},
                'summary': {}
            }
            total_violations = 0
            total_checks = 0
            for col in numeric_cols:
                col_violations = {
                    'statistical_outliers': 0,
                    'domain_violations': 0,
                    'logical_violations': 0,
                    'data_type_violations': 0
                }
                series = df[col].dropna()
                if len(series) == 0:
                    continue
                z_scores = np.abs(stats.zscore(series))
                z_outliers = (z_scores > 3).sum()
                Q1, Q3 = series.quantile([0.25, 0.75])
                IQR = Q3 - Q1
                iqr_outliers = ((series < Q1 - 1.5*IQR) | (series > Q3 + 1.5*IQR)).sum()
                col_violations['statistical_outliers'] = int(z_outliers + iqr_outliers)
                if col.lower().find('age') != -1:
                    col_violations['domain_violations'] = int(((series < 0) | (series > 120)).sum())
                if col.lower().find('percentage') != -1 or col.lower().find('pct') != -1:
                    col_violations['domain_violations'] += int(((series < 0) | (series > 100)).sum())
                if col.lower().find('price') != -1 or col.lower().find('amount') != -1 or col.lower().find('cost') != -1:
                    col_violations['domain_violations'] += int((series < 0).sum())
                if (series % 1 != 0).any():
                    col_violations['data_type_violations'] += int((series % 1 != 0).sum())
                total_violations += sum(col_violations.values())
                total_checks += len(series)
                violation_analysis['by_column'][col] = col_violations
            violation_rate = round((total_violations / total_checks) * 100, 3) if total_checks > 0 else 0
            violation_analysis['summary'] = {
                'total_violations': int(total_violations),
                'total_checks': int(total_checks),
                'violation_rate': violation_rate
            }
            return violation_analysis
        except Exception as e:
            return {'error': str(e)}

    # MAIN ULTRA-ENHANCED METRIC ENGINE
    def calculate_metrics(self, df, target_col=None, file_path=None, csv_data=None):
        """World-class, ultra-enhanced data quality metric engine."""
        df.replace({None: np.nan}, inplace=True)
        numeric_cols, categorical_cols, date_cols = self._detect_column_types(df)
        metrics = {}
        # 1. Row Count Analysis
        metrics['row_count'] = self.enhanced_row_count(df)
        # 2. File Size Analysis
        metrics['file_size'] = self.enhanced_file_size_analysis(df, file_path=file_path, csv_data=csv_data)
        # 3. Numeric Columns Analysis
        metrics['numeric_columns'] = self.enhanced_numeric_column_detection(df)
        # 4. Date Columns Analysis
        metrics['date_columns'] = self.enhanced_date_column_detection(df)
        # 5. Missing Value Analysis
        metrics['missing_values'] = self.ultra_enhanced_missing_detection(df)
        # 6. Outlier Analysis
        metrics['outliers'] = self.ultra_enhanced_outlier_detection(df, numeric_cols)
        # 7. Correlation Analysis
        metrics['correlations'] = self.ultra_enhanced_correlation_analysis(df, numeric_cols)
        # 8. Range Violation Analysis
        metrics['range_violations'] = self.ultra_enhanced_range_violation_detection(df, numeric_cols)
        # 9. Add legacy/other metrics if needed
        metrics['legacy'] = {}
        metrics['legacy']['Null_vs_NaN_Distribution'] = self.calculate_null_vs_nan_distribution(df)
        metrics['legacy']['Data_Density_Completeness'] = round(self.calculate_data_density_completeness(df), 4)
        metrics['legacy']['Domain_Constraint_Violations_%'] = round(self.enhanced_domain_validation(df), 2)
        metrics['legacy']['Data_Type_Mismatch_Rate_%'] = round(self.enhanced_data_type_validation(df), 2)
        if numeric_cols:
            metrics['legacy']['Variance_Thresh_LowVar_%'] = round(self.calculate_variance_threshold_check(df, numeric_cols) * 100, 2)
        if categorical_cols:
            metrics['legacy']['Cardinality_Categorical'] = self.calculate_cardinality_categorical(df, categorical_cols)
            metrics['legacy']['Encoding_Coverage_Rate'] = round(self.calculate_encoding_coverage_rate(df, categorical_cols), 4)
        if target_col and target_col in df.columns:
            metrics['legacy']['Target_Imbalance'] = round(self.calculate_target_imbalance(df, target_col), 4)
            metrics['legacy']['Feature_Importance_Consistency'] = round(self.calculate_feature_importance_consistency(df, numeric_cols, target_col), 4)
            metrics['legacy']['Class_Overlap_Score'] = round(self.calculate_class_overlap_score(df, numeric_cols, target_col), 4)
            metrics['legacy']['Label_Noise_Rate'] = round(self.calculate_label_noise_rate(df, target_col), 4)
        if date_cols:
            metrics['legacy']['Data_Freshness_Days'] = self.enhanced_data_freshness(df, date_cols)
        return metrics

# Ensure this is outside the class

# Ensure this is outside the class

def flatten_metrics(metrics, parent_key=None):
    """
    Recursively flatten a nested metrics dictionary into an array of {name, value} pairs.
    Nested keys will be joined with "." for clarity.
    """
    items = []
    if isinstance(metrics, dict):
        for k, v in metrics.items():
            new_key = f"{parent_key}.{k}" if parent_key else k
            if isinstance(v, dict):
                items.extend(flatten_metrics(v, new_key))
            elif isinstance(v, list):
                # Optionally handle lists of dicts (e.g., legacy metrics)
                if all(isinstance(i, dict) and 'name' in i and 'value' in i for i in v):
                    # Already in correct format
                    items.extend(v)
                else:
                    for idx, i in enumerate(v):
                        items.extend(flatten_metrics(i, f"{new_key}[{idx}]"))
            else:
                items.append({"name": new_key, "value": v})
    else:
        items.append({"name": parent_key or "metric", "value": metrics})
    return items




def create_sample_data():
    """Create sample data for GET requests when no data is provided."""
    sample_data = {
        'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve', 'Frank', 'Alice'],
        'Age': [25, 30, 35, None, 28, 45, 25],
        'Salary': [50000, 60000, 70000, 80000, -5000, 90000, 50000],
        'Department': ['IT', 'HR', 'Finance', 'IT', 'Marketing', 'Finance', 'IT'],
        'Join_Date': ['2020-01-15', '2019-03-22', '2021-06-10', '2018-11-05', '2022-02-28', '2017-09-12', '2020-01-15']
    }
    return pd.DataFrame(sample_data)

# Helper to convert numpy types to native Python types for JSON serialization
import numpy as np

def convert_numpy_types(obj):
    if isinstance(obj, dict):
        return {k: convert_numpy_types(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(item) for item in obj]
    elif isinstance(obj, (np.integer, np.int64)):
        return int(obj)
    elif isinstance(obj, (np.floating, np.float64)):
        return float(obj)
    elif isinstance(obj, (np.ndarray,)):
        return obj.tolist()
    else:
        return obj

@app.route("/analyze", methods=["GET", "POST"])
def analyze():
    """
    Endpoint to analyze CSV data and return quality metrics.
    
    POST: Expects JSON with: datasetId, datasetName, csvData
    GET: Returns sample analysis or handles query parameters
    
    Returns list of metrics in {name, value} format
    """
    
    if request.method == "POST":
        # Handle POST requests (original functionality)
        try:
            payload = request.get_json(force=True)
        except Exception:
            return jsonify({"error": "Request must be JSON"}), 400

        required_keys = {"datasetId", "datasetName", "csvData"}
        if not required_keys.issubset(payload.keys()):
            return jsonify({"error": f"Missing keys: {required_keys - payload.keys()}"}), 400

        try:
            df = pd.read_csv(StringIO(payload["csvData"]))
            print("[DEBUG] DataFrame shape:", df.shape)
            print("[DEBUG] DataFrame head:\n", df.head())
        except Exception as exc:
            return jsonify({"error": f"Unable to parse CSV: {exc}"}), 400

        # Calculate metrics using the advanced implementation
        dqm = DataQualityMetrics(dataset_id=payload["datasetId"])
        target_col = payload.get("targetColumn")  # Optional target column
        metrics = dqm.calculate_metrics(df, target_col)
        print("[DEBUG] Raw metrics dict:", metrics)
        metrics = convert_numpy_types(metrics)
        input_features = [
            "Row_Count",
            "Column_Count",
            "File_Size_MB",
            "Numeric_Columns_Count",
            "Categorical_Columns_Count",
            "Date_Columns_Count",
            "Missing_Values_Pct",
            "Duplicate_Records_Count",
            "Outlier_Rate",
            "Inconsistency_Rate",
            "Data_Type_Mismatch_Rate",
            "Null_vs_NaN_Distribution",
            "Cardinality_Categorical",
            "Target_Imbalance",
            "Feature_Importance_Consistency",
            "Class_Overlap_Score",
            "Label_Noise_Rate",
            "Feature_Correlation_Mean",
            "Range_Violation_Rate",
            "Mean_Median_Drift",
            "Data_Freshness",
            "Anomaly_Count",
            "Encoding_Coverage_Rate",
            "Variance_Threshold_Check",
            "Data_Density_Completeness",
            "Domain_Constraint_Violations"
        ]
        # Flatten all metrics
        metrics_array = flatten_metrics(metrics)
        # Build a lookup for fast access (dot notation and simple keys)
        flat_lookup = {item['name']: item['value'] for item in metrics_array}
        # Also add top-level keys
        for k, v in metrics.items():
            if not isinstance(v, dict):
                flat_lookup[k] = v
        # Also add legacy keys if present
        if 'legacy' in metrics and isinstance(metrics['legacy'], dict):
            for k, v in metrics['legacy'].items():
                flat_lookup[k] = v

        # Custom extraction logic for best available score for each metric
        def extract_metric_score(name):
            # Map of metric name to its best possible source in the nested dict
            # This is based on the structure of calculate_metrics
            mapping = {
                'Row_Count': lambda m: m.get('row_count', {}).get('total_rows'),
                'Column_Count': lambda m: m.get('row_count', {}).get('total_columns'),
                'File_Size_MB': lambda m: m.get('file_size', {}).get('actual_file_size_mb') \
    or m.get('file_size', {}).get('memory_footprint_mb') \
    or m.get('file_size', {}).get('raw_csv_size_mb') \
    or 0,
                'Numeric_Columns_Count': lambda m: m.get('numeric_columns', {}).get('summary', {}).get('total_numeric_columns'),
                'Categorical_Columns_Count': lambda m: m.get('numeric_columns', {}).get('summary', {}).get('total_categorical_columns') if m.get('numeric_columns', {}).get('summary', {}).get('total_categorical_columns') is not None else len(m.get('numeric_columns', {}).get('convertible_numeric', [])) + len(m.get('numeric_columns', {}).get('mixed_numeric', [])) + len(m.get('numeric_columns', {}).get('pseudo_numeric', [])),
                'Date_Columns_Count': lambda m: m.get('date_columns', {}).get('summary', {}).get('total_date_columns'),
                'Missing_Values_Pct': lambda m: m.get('missing_values', {}).get('summary', {}).get('overall_missing_rate'),
                'Duplicate_Records_Count': lambda m: m.get('row_count', {}).get('duplicate_rows'),
                'Outlier_Rate': lambda m: m.get('outliers', {}).get('summary', {}).get('weighted_outlier_rate'),
                'Inconsistency_Rate': lambda m: m.get('missing_values', {}).get('summary', {}).get('severely_incomplete_columns'),
                'Data_Type_Mismatch_Rate': lambda m: m.get('legacy', {}).get('Data_Type_Mismatch_Rate_%'),
                'Null_vs_NaN_Distribution': lambda m: m.get('legacy', {}).get('Null_vs_NaN_Distribution'),
                'Cardinality_Categorical': lambda m: m.get('legacy', {}).get('Cardinality_Categorical'),
                'Target_Imbalance': lambda m: (
                    m.get('legacy', {}).get('Target_Imbalance')
                    if m.get('legacy', {}).get('Target_Imbalance') is not None
                    else 0
                ),
                'Feature_Importance_Consistency': lambda m: (
    m.get('legacy', {}).get('Feature_Importance_Consistency')
    if m.get('legacy', {}).get('Feature_Importance_Consistency') is not None
    else 0
),
                'Class_Overlap_Score': lambda m: (
    m.get('legacy', {}).get('Class_Overlap_Score')
    if m.get('legacy', {}).get('Class_Overlap_Score') is not None
    else 0
),
                'Label_Noise_Rate': lambda m: (
    m.get('legacy', {}).get('Label_Noise_Rate')
    if m.get('legacy', {}).get('Label_Noise_Rate') is not None
    else 0
),
                'Feature_Correlation_Mean': lambda m: m.get('correlations', {}).get('correlation_strengths', {}).get('pearson', {}).get('mean_correlation'),
                'Range_Violation_Rate': lambda m: m.get('range_violations', {}).get('summary', {}).get('violation_rate'),
                'Mean_Median_Drift': lambda m: m.get('correlations', {}).get('summary', {}).get('correlation_complexity'),
                'Data_Freshness': lambda m: m.get('legacy', {}).get('Data_Freshness_Days'),
                'Anomaly_Count': lambda m: m.get('outliers', {}).get('summary', {}).get('consensus_outliers'),
                'Encoding_Coverage_Rate': lambda m: m.get('legacy', {}).get('Encoding_Coverage_Rate'),
                'Variance_Threshold_Check': lambda m: m.get('legacy', {}).get('Variance_Thresh_LowVar_%'),
                'Data_Density_Completeness': lambda m: m.get('legacy', {}).get('Data_Density_Completeness'),
                'Domain_Constraint_Violations': lambda m: m.get('legacy', {}).get('Domain_Constraint_Violations_%'),
            }
            func = mapping.get(name)
            if func:
                try:
                    value = func(metrics)
                    if value is not None:
                        return value
                except Exception:
                    pass
            # fallback: try flattened lookup
            return flat_lookup.get(name)

        filtered_metrics = []
        for feature in input_features:
            value = extract_metric_score(feature)
            if value is None:
                print(f"[DEBUG] Metric '{feature}' could not be computed or is missing in metrics dict.")
            filtered_metrics.append({'name': feature, 'value': value})
        print("[DEBUG] Returning metrics:", filtered_metrics)
        return jsonify(filtered_metrics), 200
    
    elif request.method == "GET":
        # Handle GET requests (fallback for frontend compatibility)
        try:
            # Check if query parameters are provided
            args = request.args
            print(f"GET request received with args: {args.to_dict()}")
            
            # Check if CSV data is provided via query parameters
            csv_data = args.get('csvData')
            dataset_id = args.get('datasetId', 'sample_dataset')
            
            if csv_data:
                try:
                    df = pd.read_csv(StringIO(csv_data))
                except Exception as exc:
                    return jsonify({"error": f"Unable to parse CSV from query: {exc}"}), 400
            else:
                # Use sample data for demonstration
                df = create_sample_data()
                print("Using sample data for GET request")
            
            # Calculate metrics
            dqm = DataQualityMetrics(dataset_id=dataset_id)
            target_col = args.get('targetColumn')
            metrics = dqm.calculate_metrics(df, target_col)
            metrics = convert_numpy_types(metrics)
            input_features = [
                "Row_Count",
                "Column_Count",
                "File_Size_MB",
                "Numeric_Columns_Count",
                "Categorical_Columns_Count",
                "Date_Columns_Count",
                "Missing_Values_Pct",
                "Duplicate_Records_Count",
                "Outlier_Rate",
                "Inconsistency_Rate",
                "Data_Type_Mismatch_Rate",
                "Null_vs_NaN_Distribution",
                "Cardinality_Categorical",
                "Target_Imbalance",
                "Feature_Importance_Consistency",
                "Class_Overlap_Score",
                "Label_Noise_Rate",
                "Feature_Correlation_Mean",
                "Range_Violation_Rate",
                "Mean_Median_Drift",
                "Data_Freshness",
                "Anomaly_Count",
                "Encoding_Coverage_Rate",
                "Variance_Threshold_Check",
                "Data_Density_Completeness",
                "Domain_Constraint_Violations"
            ]
            metrics_array = flatten_metrics(metrics)
            filtered_metrics = [item for item in metrics_array if item["name"] in input_features]
            return jsonify(filtered_metrics), 200
            
        except Exception as exc:
            return jsonify({
                "error": f"GET request failed: {exc}",
                "method": "GET",
                "fallback_available": True
            }), 500

@app.route("/health", methods=["GET"])
def health_check():
    """Health check endpoint."""
    return jsonify({
        "status": "healthy",
        "message": "Data Quality Metrics API is running",
        "supported_methods": ["GET", "POST"],
        "endpoints": ["/analyze", "/health"]
    }), 200

@app.route("/", methods=["GET"])
def root():
    """Root endpoint with API information."""
    return jsonify({
        "name": "Data Quality Metrics API",
        "version": "1.0.0",
        "description": "Comprehensive API for analyzing data quality metrics in CSV datasets",
        "endpoints": {
            "/analyze": {
                "methods": ["GET", "POST"],
                "description": "Analyze CSV data and return quality metrics"
            },
            "/health": {
                "methods": ["GET"],
                "description": "Health check endpoint"
            }
        },
        "usage": {
            "POST /analyze": {
                "content-type": "application/json",
                "body": {
                    "datasetId": "string",
                    "datasetName": "string", 
                    "csvData": "string (CSV content)",
                    "targetColumn": "string (optional)"
                }
            },
            "GET /analyze": {
                "description": "Fallback method, uses sample data or query parameters",
                "query_params": {
                    "datasetId": "optional",
                    "csvData": "optional CSV content",
                    "targetColumn": "optional"
                }
            }
        }
    }), 200

if __name__ == "__main__":
    print("Starting Data Quality Metrics API...")
    print("Available endpoints:")
    print("  GET/POST /analyze - Analyze CSV data")
    print("  GET /health - Health check")
    print("  GET / - API information")
    app.run(host="0.0.0.0", port=1289, debug=True)  